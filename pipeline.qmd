---
title: "Model pipeline"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      comment = "#>",
                      tar_interactive = interactive())
library(targets)
tar_unscript()
```

## Intro

We use the [{targets}](https://books.ropensci.org/targets/) package to manage
the code pipeline used for running the model.

See [setup](setup.qmd) for more info.

In addition to the targets package itself we make use of two targets-related
packages: [tarchetypes](https://docs.ropensci.org/tarchetypes/index.html) and
[stantargets](https://docs.ropensci.org/stantargets/index.html).

```{r}
library(targets)
library(tarchetypes)
library(stantargets)
```

## Globals

We first define some global options/functions common to all targets. The function below plots a histogram of ozone concentrations, and our histogram target will need it.

```{targets example-globals, tar_globals = TRUE}
library(targets)
library(tarchetypes)
library(stantargets)
options(tidyverse.quiet = TRUE)
tar_source() # grab all functions in R folder
tar_option_set(packages = c(
  "readr",
  "tidyr",
  "rvest",
  "glue",
  "dplyr",
  "stringr",
  "lubridate",
  "forcats"
))
```

## Targets


```{targets urls}
list(
  tar_url(polls2011_url, wikipedia_poll_url(2011)),
  tar_url(polls2014_url, wikipedia_poll_url(2014)),
  tar_url(polls2017_url, wikipedia_poll_url(2017)),
  tar_url(polls2020_url, wikipedia_poll_url(2020)),
  tar_url(polls2023_url, wikipedia_poll_url(2023)),
  tar_url(results2008_url, election_results_summary(2008)),
  tar_url(results2011_url, election_results_summary(2011)),
  tar_url(results2014_url, election_results_summary(2014)),
  tar_url(results2017_url, election_results_summary(2017)),
  tar_url(results2020_url, election_results_summary(2020))
)
```




```{targets polls}
list(
  tar_target(polls2011, extract_poll_results_2011(polls2011_url)),
  tar_target(polls2014, extract_poll_results_2014(polls2014_url)),
  tar_target(polls2017, extract_poll_results_2017(polls2017_url)),
  tar_target(polls2020, extract_poll_results_2020(polls2020_url)),
  tar_target(polls2023, extract_poll_results_2023(polls2023_url)),
  tar_target(polls, combine_polls(polls2011,polls2014,polls2017,polls2020,polls2023))
)
```

```{targets results}
list(
  tar_target(results2020, fetch_results(results2020_url, 17, 2020)),
  tar_target(results2017, fetch_results(results2017_url, 16, 2017)),
  tar_target(results2014, fetch_results(results2014_url, 15, 2014)),
  tar_target(results2011, fetch_results(results2011_url, 13, 2011)),
  tar_target(results2008, fetch_results(results2008_url, 19, 2008)),
  tar_target(
    results,
    combine_results(
      results2008,
      results2011,
      results2014,
      results2017,
      results2020
    )
  )
  
)
```

### Selecting parties and pollsters

Only include pollsters who have provided a poll for the 2023 election. (Not sure
how the model will handle only having Curia polls after the 2020 election - may
need to remove it and consider adding leaked Curia polls - or talking to Curia.)

Include parties who currently have a seat in parliament or who have polled over
2.5% three times.

```{targets params}
list(
  tar_target(pollsters, polls |> 
               filter(Election == 2023) |> 
               distinct(Pollster) |> 
               filter(!grepl('Roy|Horizon', Pollster))),
  tar_target(parties_in_parliament, 
             tibble(Party = c('ACT', 'Green', 'Labour', 'National', 'Te Pāti Māori'))),
  tar_target(parties,  polls |> 
               filter(Election == 2023, VotingIntention > 0.025) |>
               filter(n() > 3, .by=Party) |> 
               distinct(Party) |> 
               union(parties_in_parliament) |> 
               arrange(Party)),
  tar_target(pollsters2020, polls |> 
               filter(Election == 2020,
                      Pollster != 'YouGov') |> # Only one YouGov poll 
               distinct(Pollster)),
  tar_target(parties_in_parliament2020, 
             tibble(Party = c('ACT', 'Green', 'Labour', 'National', 'NZ First'))),
  tar_target(parties2020,  polls |> 
               filter(Election == 2020, VotingIntention > 0.025) |>
               filter(n() > 3, .by=Party) |> 
               distinct(Party) |> 
               union(parties_in_parliament2020) |> 
               arrange(Party))
)
```

```{targets prep}
list(
  tar_target(election_weeks, floor_date(ymd(
    c("2014-09-20", "2017-09-23", "2020-10-17", "2023-10-14")
  ), unit = 'week')),
  tar_target(weeks_between_elections, as.integer(diff(election_weeks)) / 7),
  tar_target(
    elections,
    results |>
      filter(Election >= year(min(election_weeks))) |>
      mutate(
        Party = fct_other(Party, keep = parties$Party) |>
          fct_relevel(parties$Party)
      ) |>
      count(Party, Election, wt = Percentage, name = 'Percentage') |>
      arrange(Party, Election) |>
      pivot_wider(names_from = Party, values_from = Percentage, values_fill = 0) |>
      select(-Election)
  ),
  tar_target(
    polls2,
    polls |>
  filter(Pollster %in% c(pollsters$Pollster), !is.na(MidPoint)) |>
  mutate(
    MidPoint = floor_date(MidPoint, unit = 'week'),
    Party = fct_other(Party, keep = parties$Party) |>
      fct_relevel(parties$Party)
  ) |>
  filter(Party != 'Other') |> 
  mutate(Polled = sum(VotingIntention), .by = c(Pollster, MidPoint)) |> 
  rename(MidDate = MidPoint, ElectionYear = Election) |>
  count(
    Party,
    Pollster,
    MidDate,
    ElectionYear,
    Polled,
    wt = VotingIntention,
    name = "VotingIntention"
  ) |>
  filter(ElectionYear %in% year(election_weeks[-1])) |>
  arrange(Party, MidDate) |>
  pivot_wider(
    names_from = Party,
    values_from = VotingIntention,
    names_sort = TRUE,
    values_fill = 0
  ) |>
  mutate(Other = pmax(0, 1 - Polled),
         MidDateNumber = 1 + as.numeric(MidDate - election_weeks[1]) / 7) |> 
  select(-Polled)

  ),
  tar_target(
    polls3,
    polls2 |>
      arrange(Pollster, MidDate) |>
      group_split(Pollster)
  ),
  tar_target(parties_ss, names(elections)),
  tar_target(
    # estimate the standard errors.  Note we are pretending they all have a sample size of 1000 -
    # which the main five do, but not some of the smaller ones.  Improvement would be to better deal with this.
    
    all_ses,
    polls2 |>
      select(Pollster, ACT:Other) |>
      pivot_longer(ACT:Other, names_to = 'Party', values_to = 'p') |>
      summarise(
        p = mean(p, na.rm = T),
        se = sqrt(p * (1 - p) / 1000),
        .by = c(Pollster, Party)
      )
  ),
  tar_target(
    ses3,
    all_ses |>
      arrange(Pollster, Party) |>
      group_split(Pollster)
  ),
  tar_target(
    d1, list(mu_elect1 = as.numeric(elections[1, ]), 
           mu_elect2 = as.numeric(elections[2, ]),
           mu_elect3 = as.numeric(elections[3, ]), 

           n_parties = length(parties_ss),
           n_weeks = weeks_between_elections, 
           # multiply the variance of all polls by 2.  See my blog post of 9 July 2017.
           inflator = sqrt(2),
           
           y1_n = nrow(polls3[[1]]),
           y1_values = polls3[[1]][ , 4:11],
           y1_weeks = as.numeric(polls3[[1]]$MidDateNumber),
           y1_se = ses3[[1]]$se,
           
           y2_n = nrow(polls3[[2]]),
           y2_values = polls3[[2]][ , 4:11],
           y2_weeks = as.numeric(polls3[[2]]$MidDateNumber),
           y2_se = ses3[[2]]$se,
           
           y3_n = nrow(polls3[[3]]),
           y3_values = polls3[[3]][ , 4:11],
           y3_weeks = as.numeric(polls3[[3]]$MidDateNumber),
           y3_se = ses3[[3]]$se,
           
           y4_n = nrow(polls3[[4]]),
           y4_values = polls3[[4]][ , 4:11],
           y4_weeks = as.numeric(polls3[[4]]$MidDateNumber),
           y4_se = ses3[[4]]$se,
           reid_method = as.numeric(polls3[[4]]$MidDate >= as.Date("2017-01-01")),
           
           n_pollsters = 4)
  )
)

```


```{targets prep2020}
list(
  tar_target(election_weeks2020, floor_date(ymd(
    c("2011-11-26", "2014-09-20", "2017-09-23", "2020-10-17")
  ), unit = 'week')),
  tar_target(weeks_between_elections2020, as.integer(diff(election_weeks)) / 7),
  tar_target(
    elections2020,
    results |>
      filter(Election >= year(min(election_weeks2020)) & 
               Election < year(max(election_weeks2020))) |>
      mutate(
        Party = fct_other(Party, keep = parties2020$Party) |>
          fct_relevel(parties2020$Party)
      ) |>
      count(Party, Election, wt = Percentage, name = 'Percentage') |>
      arrange(Party, Election) |>
      pivot_wider(names_from = Party, values_from = Percentage, values_fill = 0) |>
      select(-Election)
  ),
  tar_target(
    polls22020,
    polls |>
      filter(Election %in% c(2014, 2017, 2020),
             Pollster %in% c(pollsters2020$Pollster),
             !is.na(MidPoint)) |>
  mutate(
    MidPoint = floor_date(MidPoint, unit = 'week'),
    Party = fct_other(Party, keep = parties2020$Party) |>
      fct_relevel(parties2020$Party)
  ) |>
  filter(Party != 'Other') |> 
  mutate(Polled = sum(VotingIntention), .by = c(Pollster, MidPoint)) |> 
  rename(MidDate = MidPoint, ElectionYear = Election) |>
  count(
    Party,
    Pollster,
    MidDate,
    ElectionYear,
    Polled,
    wt = VotingIntention,
    name = "VotingIntention"
  ) |>
  filter(MidDate <= election_weeks2020[4]) |>
  arrange(Party, MidDate) |>
  pivot_wider(
    names_from = Party,
    values_from = VotingIntention,
    names_sort = TRUE,
    values_fill = 0
  ) |>
  mutate(Other = pmax(0, 1 - Polled),
         MidDateNumber = 1 + as.numeric(MidDate - election_weeks2020[1]) / 7) |> 
  select(-Polled)

  ),
  tar_target(
    polls32020,
    polls22020 |>
      arrange(Pollster, MidDate) |>
      group_split(Pollster)
  ),
  tar_target(parties_ss2020, names(elections2020)),
  tar_target(
    # estimate the standard errors.  Note we are pretending they all have a sample size of 1000 -
    # which the main five do, but not some of the smaller ones.  Improvement would be to better deal with this.
    
    all_ses2020,
    polls22020 |>
      select(Pollster, ACT:Other) |>
      pivot_longer(ACT:Other, names_to = 'Party', values_to = 'p') |>
      summarise(
        p = mean(p, na.rm = T),
        se = sqrt(p * (1 - p) / 1000),
        .by = c(Pollster, Party)
      )
  ),
  tar_target(
    ses32020,
    all_ses2020 |>
      arrange(Pollster, Party) |>
      group_split(Pollster)
  ),
  tar_target(
    d12020, list(mu_elect1 = as.numeric(elections2020[1, ]), 
           mu_elect2 = as.numeric(elections2020[2, ]),
           mu_elect3 = as.numeric(elections2020[3, ]), 

           n_parties = length(parties_ss2020),
           n_weeks = weeks_between_elections2020, 
           # multiply the variance of all polls by 2.  See my blog post of 9 July 2017.
           inflator = sqrt(2),
           
           y1_n = nrow(polls32020[[1]]),
           y1_values = polls32020[[1]][ , 4:9],
           y1_weeks = as.numeric(polls32020[[1]]$MidDateNumber),
           y1_se = ses32020[[1]]$se,
           
           y2_n = nrow(polls32020[[2]]),
           y2_values = polls32020[[2]][ , 4:9],
           y2_weeks = as.numeric(polls32020[[2]]$MidDateNumber),
           y2_se = ses32020[[2]]$se,
           reid_method = as.numeric(polls32020[[2]]$MidDate >= as.Date("2017-01-01")),
           
           y3_n = nrow(polls32020[[3]]),
           y3_values = polls32020[[3]][ , 4:9],
           y3_weeks = as.numeric(polls32020[[3]]$MidDateNumber),
           y3_se = ses32020[[3]]$se,
           
           n_pollsters = 3)
  )
)

```


```{targets modelling}
list(
  tar_stan_mcmc(
    model2023,
    "stan/model2023.stan",
    dir = ".stan",
    data = d1,
    chains = 4,
    parallel_chains = 4,
    iter_sampling = 2000,
    max_treedepth = 20,
    deployment = "worker"
  ),
    tar_stan_mcmc(
    model2020,
    "stan/model2020.stan",
    dir = ".stan",
    data = d12020,
    chains = 4,
    parallel_chains = 4,
    iter_sampling = 2000,
    max_treedepth = 20,
    deployment = "worker"
  )

)
```


